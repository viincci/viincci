{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9zQxobxF5FNsL+xlNb9bK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viincci/viincci/blob/main/.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu_Mei5AyXEq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[tourch] datasets sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "SvA0-icOygj-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recipe = pd.read_csv(open(\"recipe.csv\",\"rb\")).drop(['url',  'author', 'rating','rating_count', 'review_count', 'prep_time', 'cook_time',  'servings', 'calories', 'carbohydrates_g', 'sugars_g', 'fat_g', 'saturated_fat_g','cholesterol_mg', 'protein_g', 'dietary_fiber_g', 'sodium_mg','calories_from_fat', 'calcium_mg', 'iron_mg', 'magnesium_mg','potassium_mg', 'zinc_mg', 'phosphorus_mg', 'vitamin_a_iu_IU','niacin_equivalents_mg', 'vitamin_b6_mg', 'vitamin_c_mg', 'folate_mcg','thiamin_mg', 'riboflavin_mg', 'vitamin_e_iu_IU', 'vitamin_k_mcg','biotin_mcg', 'vitamin_b12_mcg', 'mono_fat_g', 'poly_fat_g','trans_fatty_acid_g', 'omega_3_fatty_acid_g', 'omega_6_fatty_acid_g','instructions_list', 'image'], axis=1).to_dict()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_XT6ySBvHCWB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(instructions,responses):\n",
        "    lst = {}\n",
        "    for i , k in enumerate(instructions):\n",
        "        if k in lst.keys():\n",
        "            lst[f'{k}'].append(i)\n",
        "        elif k not in lst.keys():\n",
        "            lst[k] = [i]\n",
        "    newdct = {}\n",
        "    for k , v in lst.items():\n",
        "        anw = \"\"\n",
        "        for i in v:\n",
        "            anw += responses[i]\n",
        "        newdct[k] = anw\n",
        "    # print(f\"{[x for x in newdct.keys()][-1]} : {newdct.get([x for x in lst.keys()][-1])}\")\n",
        "    return newdct\n",
        "\n",
        "def getdata():\n",
        "    data = {}\n",
        "    # for i,k in clean_data(dataset['validation']['question'],dataset['validation']['answer']).items():\n",
        "    #     data[i] = k\n",
        "    # sem = json.load(open('datasets/sem01.json','r'))\n",
        "    # for i,k in sem.items():\n",
        "    #     data[i] = k\n",
        "\n",
        "    _dct = []\n",
        "    # data = {}\n",
        "    for k,v in recipe['title'].items():\n",
        "        name = v\n",
        "        make = f\"{v} is a {recipe['category'].get(k)} \\n {recipe['description'].get(k)} it take {recipe['total_time'].get(k)} to cook this meal \\n this meal yields {recipe['yields'].get(k)} \\n Ingredients {recipe['ingredients'].get(k)} \\n Directions {recipe['directions'].get(k)}  \"\n",
        "        data[name] = make\n",
        "    for k, v in data.items():\n",
        "        _dct.append({\"Question\": k, \"Answer\" : v })\n",
        "\n",
        "    # Combine the datasets into a single DataFrame\n",
        "    dataframe = pd.json_normalize(_dct)\n",
        "    # json.dump(data', 'pen('datasets/sem01.json','w'))\n",
        "    return dataframe"
      ],
      "metadata": {
        "id": "EPszJHUnHVOE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"google/flan-t5-small\"\n",
        "model_name = f\"{path}\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "# Preprocess and tokenize the dataset\n",
        "def tokenize_batch(batch):\n",
        "    source_texts = batch[\"Question\"]\n",
        "    target_texts = batch[\"Answer\"]\n",
        "    tokenized_source = tokenizer.batch_encode_plus(\n",
        "        source_texts,\n",
        "        pad_to_max_length=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=128,\n",
        "    )\n",
        "    tokenized_target = tokenizer.batch_encode_plus(\n",
        "        target_texts,\n",
        "        pad_to_max_length=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=128,\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": tokenized_source[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized_source[\"attention_mask\"],\n",
        "        \"labels\": tokenized_target[\"input_ids\"],\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "tA7NsZTwHcLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output_dir\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        ")\n"
      ],
      "metadata": {
        "id": "1iRy441gHpFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = Dataset.from_pandas(getdata()).train_test_split(shuffle=True)\n",
        "# Assuming \"train\" is the split name for training data in the wiki_qa dataset\n",
        "train_dataset = combined_dataset[\"train\"].map(tokenize_batch, batched=True,batch_size=1500)\n",
        "eval_dataset = combined_dataset[\"test\"].map(tokenize_batch, batched=True,batch_size=1500)  # Create an evaluation dataset\n"
      ],
      "metadata": {
        "id": "Fm-_UMntH776"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # Provide the evaluation dataset\n",
        ")\n"
      ],
      "metadata": {
        "id": "TrruNlftH_Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "il0XmnGlIBaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save_pretrained(\"./ChefSemblanceFlanT5Base\")\n",
        "tokenizer.save_pretrained(\"./ChefSemblanceFlanT5Base\")"
      ],
      "metadata": {
        "id": "M2F7d3PzIFvi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}